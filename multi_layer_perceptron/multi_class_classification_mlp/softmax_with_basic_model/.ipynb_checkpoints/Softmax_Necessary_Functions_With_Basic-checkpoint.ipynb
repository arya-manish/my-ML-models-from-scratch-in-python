{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### One Hot Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(num_class, arr):\n",
    "    one_hot = np.zeros((num_class, len(arr)))\n",
    "    for i in range(0,len(arr)):\n",
    "        temp = arr[i]\n",
    "        one_hot[temp][i] = 1\n",
    "    return one_hot    \n",
    "        \n",
    "        \n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ReLu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ReLu\n",
    "def relu(x):\n",
    "    \n",
    "   #  Arguments:\n",
    "   #x -- A scalar or numpy array of any size.\n",
    "    \n",
    "    r = np.maximum(0,x)\n",
    "    \n",
    "    return r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(Z):\n",
    "    expZ = np.exp(Z)\n",
    "    expZsum = np.sum(expZ, axis = 0,keepdims = True)\n",
    "    softmax = expZ/expZsum\n",
    "    return softmax\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute Cost for Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def compute_cost_softmax(AL, Y):\n",
    "        m = Y.shape[1]\n",
    "        cost = -(1/m)*np.nansum(np.nansum(np.multiply(Y, np.log(AL)), axis = 0,keepdims = True))\n",
    "        return cost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#AL = softmax(np.random.randn(5,8))\n",
    "#Y = one_hot(5,np.array([1,0,4,3,1,3,2,1]))\n",
    "#cost = compute_cost_softmax(Y,AL)\n",
    "#print(cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### He initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def He_initialize_parameters(layers_dims):\n",
    "    L = len(layers_dims)\n",
    "    parameters = {}\n",
    "    for k in range(1,L):\n",
    "        parameters['W'+str(k)] = (np.random.randn(layers_dims[k],layers_dims[k-1]))*np.sqrt(2/layers_dims[k-1])\n",
    "        parameters['b' +str(k)] = np.zeros((layers_dims[k],1))\n",
    "    return parameters    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward Propagation (   linear -> relu ->linear ->relu.....->linear ->softmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_prop(X, parameters):   # linear -> relu ->linear ->relu.....->linear ->softmax\n",
    "    \n",
    "    L = int((1/2)*len(parameters))\n",
    "    globals()['A'+ str(0)] = X\n",
    "    for l in range(1, L):  # linear->relu upto L-1 layer\n",
    "        globals()['Z'+ str(l)] = np.dot(parameters['W'+str(l)], (globals()['A'+ str(l-1)])) + parameters['b' +str(l)]\n",
    "        globals()['A'+ str(l)] = relu((globals()['Z'+ str(l)]))\n",
    "    \n",
    "    globals()['Z'+ str(L)] = np.dot(parameters['W'+str(L)], (globals()['A'+ str(L-1)])) + parameters['b' +str(L)] \n",
    "    globals()['A'+ str(L)] = softmax((globals()['Z'+ str(L)]))                                \n",
    "    # storing values in cache\n",
    "    lst = []   # creating list and adding Z1, A1, W1, b1... then converting it into a tuple\n",
    "    \n",
    "    for l in range(1,L+1):                                  \n",
    "        lst.append(globals()['Z'+ str(l)])\n",
    "        lst.append(globals()['A'+ str(l)])                              \n",
    "        lst.append(parameters['W' +str(l)])\n",
    "        lst.append(parameters['b' +str(l)]) \n",
    "    cache = tuple(lst)\n",
    "    \n",
    "    return (globals()['A'+ str(L)]), cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Backward Propagation (linear -> relu ->linear ->relu.....->linear ->softmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# relu ->relu ->......->relu ->softmax\n",
    "# cache has order ( Z1, A1, W1, b1, Z2,, A2, W2, b2...)\n",
    "def back_prop_without_regularization(X, Y, cache):  \n",
    "    L = int((len(cache))/4)\n",
    "    m = X.shape[1]\n",
    "    globals()['A'+str(0)] = X\n",
    "    for i in range(1,L+1):   # retrieving the cache elements\n",
    "        globals()['Z'+ str(i)]  =  cache[4*i - 4]\n",
    "        globals()['A' + str(i)] =  cache[4*i - 3]\n",
    "        globals()['W' +str(i)]  =   cache[4*i - 2]\n",
    "        globals()['b' +str(i)]  =   cache[4*i - 1]\n",
    "    # back prop equations for last element , last layer uses sigmoid\n",
    "    globals()['dZ' + str(L)] = (1/m)*((globals()['A' + str(L)]) - Y) # I have proved that, for softmax and sigmoid, this equation remains same\n",
    "    globals()['dW' + str(L)] = np.dot((globals()['dZ' + str(L)]), (globals()['A' + str(L -1)]).T)\n",
    "    globals()['db' + str(L)] = np.sum((globals()['dZ' + str(L)]), axis = 1, keepdims = True)\n",
    "    globals()['dA' + str(L-1)]  = np.dot((globals()['W' +str(i)]).T , (globals()['dZ' + str(L)]))\n",
    "    \n",
    "    # back prop equations for layers L-1, L-2, ....1\n",
    "    for l in range(L-1, 0, -1):\n",
    "        globals()['dZ' + str(l)] = np.multiply((globals()['dA' + str(l)]), np.int64((globals()['A' + str(l)])>0))\n",
    "        globals()['dW' + str(l)] = np.dot((globals()['dZ' + str(l)]), (globals()['A' + str(l -1)]).T)\n",
    "        globals()['db' + str(l)] = np.sum((globals()['dZ' + str(l)]), axis = 1, keepdims = True)\n",
    "        globals()['dA' + str(l-1)] = np.dot((globals()['W' +str(l)]).T , (globals()['dZ' + str(l)]))\n",
    "\n",
    "    # updating gradients in dictionary\n",
    "    gradients = {}\n",
    "    for i in range(1, L+1):\n",
    "        gradients[('dW' +str(i))] = globals()['dW' +str(i)] \n",
    "        gradients[('db' +str(i))] = globals()['db' +str(i)]\n",
    "    \n",
    "    return gradients    \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient Descent Update Rule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent_update(parameters, gradients, learning_rate):\n",
    "    L = int(len(parameters)/2)\n",
    "    for l in range (1,L+1):\n",
    "        parameters['W' +str(l)] = parameters['W' + str(l)] - learning_rate*gradients['dW' + str(l)]\n",
    "        parameters['b' +str(l)] = parameters['b' + str(l)] - learning_rate*gradients['db' +str(l)]\n",
    "    return parameters   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predict Accuracy with SoftMax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred_accuracy_softmax(X,Y,parameters):\n",
    "    AL, _ =       forward_prop(X, parameters)\n",
    "    ALmax_index = np.argmax(AL, axis =0)\n",
    "    Y_hat =       one_hot(AL.shape[0], ALmax_index )\n",
    "    m =           Y.shape[1]\n",
    "    predict_accuracy = (1/m)* np.sum(np.sum(np.multiply(Y, Y_hat), axis = 0))\n",
    "    return predict_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Basic Softmax Model (with Gradient Descent, no mini batch, no regularization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_model_softmax(X, Y, learning_rate, num_iterations, layers_dims): # simple model without minibatch, only gradient descent\n",
    "    parameters = He_initialize_parameters(layers_dims)\n",
    "    costs = []\n",
    "    \n",
    "    for i in range(0, num_iterations):\n",
    "        AL, cache  = forward_prop(X, parameters)\n",
    "        cost = compute_cost_softmax(AL, Y)\n",
    "        if i%100==0:\n",
    "            print('cost after ' +str(i) +' iterations is ' + str(cost))\n",
    "        if i%10 ==0:\n",
    "            costs.append(cost)\n",
    "        grads  = back_prop_without_regularization(X, Y, cache)\n",
    "        parameters = gradient_descent_update(parameters, grads, learning_rate)\n",
    "        \n",
    "    plt.plot(np.squeeze(costs))\n",
    "    plt.ylabel('cost')\n",
    "    plt.xlabel('iterations (per tens)')\n",
    "    plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "    plt.show()\n",
    "    \n",
    "    return parameters    \n",
    "\n",
    "    \n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
