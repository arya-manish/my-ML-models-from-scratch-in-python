{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ReLu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ReLu\n",
    "def relu(x):\n",
    "    \n",
    "   #  Arguments:\n",
    "   #x -- A scalar or numpy array of any size.\n",
    "    \n",
    "    r = np.maximum(0,x)\n",
    "    \n",
    "    return r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sigmoid\n",
    "def sigmoid(x):\n",
    "    \n",
    "    #  Arguments:\n",
    "    #x -- A scalar or numpy array of any size.\n",
    "    \n",
    "    s = 1/(1+np.exp(-x))\n",
    "    return s\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def He_initialize_parameters(layers_dims):\n",
    "    L = len(layers_dims)\n",
    "    parameters = {}\n",
    "    for k in range(1,L):\n",
    "        parameters['W'+str(k)] = (np.random.randn(layers_dims[k],layers_dims[k-1]))*np.sqrt(2/layers_dims[k-1])\n",
    "        parameters['b' +str(k)] = np.zeros((layers_dims[k],1))\n",
    "    return parameters    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute Cost for sigmoid with Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regularized_compute_cost_sigmoid(AL, Y, parameters, lambd):\n",
    "    m = Y.shape[1]\n",
    "    L = int((1/2)*len(parameters))\n",
    "    sumWsquared = 0.\n",
    "    for l in range(1,L+1):\n",
    "        sumWsquared = sumWsquared + np.sum(np.square(parameters['W'+str(l)]))\n",
    "    J_Cross_Entropy = -(1/m)*np.nansum((np.multiply(Y, np.log(AL)) + np.multiply((1 - Y), np.log(1 - AL))))\n",
    "    J_Regularized_term = (lambd/(2*m))*sumWsquared\n",
    "    J = J_Cross_Entropy + J_Regularized_term                                    \n",
    "    return J"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Z = np.random.randn(1,10)\n",
    "#AL = sigmoid(Z)\n",
    "#Y = np.array([[1,0,1,1,1,0,0,1,0,1]])\n",
    "#print(AL.shape)\n",
    "#print(Y.shape)\n",
    "#lambd = 0.6\n",
    "#layers_dims = [100,20,15,1]\n",
    "#parameters=  He_initialize_parameters(layers_dims)\n",
    "#cost = regularized_compute_cost_sigmoid(AL, Y, parameters, lambd)\n",
    "#print(cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute Cost for sigmoid (same as for basic sigmoid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost_sigmoid(AL, Y):\n",
    "    m = Y.shape[1]\n",
    "    J = -(1/m)*np.nansum((np.multiply(Y, np.log(AL)) + np.multiply((1 - Y), np.log(1 - AL))))                                   \n",
    "    return J"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward Propagation (   linear -> relu ->linear ->relu.....->linear ->sigmoid) same for Reguarized and non regularized version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_prop(X, parameters):   # linear -> relu ->linear ->relu.....->linear ->sigmoid\n",
    "    \n",
    "    L = int((1/2)*len(parameters))\n",
    "    globals()['A'+ str(0)] = X\n",
    "    for l in range(1, L):  # linear->relu upto L-1 layer\n",
    "        globals()['Z'+ str(l)] = np.dot(parameters['W'+str(l)], (globals()['A'+ str(l-1)])) + parameters['b' +str(l)]\n",
    "        globals()['A'+ str(l)] = relu((globals()['Z'+ str(l)]))\n",
    "    \n",
    "    globals()['Z'+ str(L)] = np.dot(parameters['W'+str(L)], (globals()['A'+ str(L-1)])) + parameters['b' +str(L)] \n",
    "    globals()['A'+ str(L)] = sigmoid((globals()['Z'+ str(L)]))                                \n",
    "    # storing values in cache\n",
    "    lst = []   # creating list and adding Z1, A1, W1, b1... then converting it into a tuple\n",
    "    \n",
    "    for l in range(1,L+1):                                  \n",
    "        lst.append(globals()['Z'+ str(l)])\n",
    "        lst.append(globals()['A'+ str(l)])                              \n",
    "        lst.append(parameters['W' +str(l)])\n",
    "        lst.append(parameters['b' +str(l)]) \n",
    "    cache = tuple(lst)\n",
    "    \n",
    "    return (globals()['A'+ str(L)]), cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward Propagation (   linear -> relu ->linear ->relu.....->linear ->sigmoid) with Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_prop_with_dropout(X, parameters, keep_prob):   # linear -> relu ->linear ->relu.....->linear ->sigmoid\n",
    "    \n",
    "    np.random.seed(1)\n",
    "    \n",
    "    L = int((1/2)*len(parameters))\n",
    "    globals()['A'+ str(0)] = X\n",
    "    for l in range(1, L):  # linear->relu upto L-1 layer\n",
    "        globals()['Z'+ str(l)] = np.dot(parameters['W'+str(l)], (globals()['A'+ str(l-1)])) + parameters['b' +str(l)]\n",
    "        globals()['A'+ str(l)] = relu((globals()['Z'+ str(l)]))\n",
    "        # dropout equations on the hidden layers and not on the input and output layers\n",
    "        globals()['DA' + str(l)] = np.random.rand((globals()['A'+str(l)]).shape[0],(globals()['A'+str(l)]).shape[1]) # DA1, DA2 etc. stands for dropout A1, A2 etc   \n",
    "        globals()['DA' + str(l)] = globals()['DA' + str(l)] < keep_prob\n",
    "        globals()['A'+ str(l)] = np.multiply(globals()['A'+ str(l)], globals()['DA' + str(l)])\n",
    "        globals()['A'+ str(l)] =  (globals()['A'+ str(l)]) /keep_prob\n",
    "        \n",
    "    globals()['Z'+ str(L)] = np.dot(parameters['W'+str(L)], (globals()['A'+ str(L-1)])) + parameters['b' +str(L)] \n",
    "    globals()['A'+ str(L)] = sigmoid((globals()['Z'+ str(L)]))                                \n",
    "    # storing values in cache\n",
    "    lst = []   # creating list and adding Z1, A1, W1, b1... then converting it into a tuple\n",
    "    \n",
    "    for l in range(1,L):                                  \n",
    "        lst.append(globals()['Z'+ str(l)])\n",
    "        lst.append(globals()['DA'+ str(l)])\n",
    "        lst.append(globals()['A'+ str(l)])\n",
    "        lst.append(parameters['W' +str(l)])\n",
    "        lst.append(parameters['b' +str(l)])\n",
    "    lst.append(globals()['Z'+ str(L)])\n",
    "    lst.append(globals()['A'+ str(L)])\n",
    "    lst.append(parameters['W' +str(L)])\n",
    "    lst.append(parameters['b' +str(L)])     \n",
    "    cache = tuple(lst)\n",
    "    \n",
    "    return (globals()['A'+ str(L)]), cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backward Propagation (linear -> relu ->linear ->relu.....->linear ->sigmoid) with No Regularization and No Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# relu ->relu ->......->relu ->sigmoid\n",
    "# cache has order ( Z1, A1, W1, b1, Z2,, A2, W2, b2...)\n",
    "def back_prop_without_regularization(X, Y, cache):  \n",
    "    L = int((len(cache))/4)\n",
    "    m = X.shape[1]\n",
    "    globals()['A'+ str(0)] = X\n",
    "    for i in range(1,L+1):   # retrieving the cache elements\n",
    "        globals()['Z'+ str(i)]  =  cache[4*i - 4]\n",
    "        globals()['A' + str(i)] =  cache[4*i - 3]\n",
    "        globals()['W' +str(i)]  =   cache[4*i - 2]\n",
    "        globals()['b' +str(i)]  =   cache[4*i - 1]\n",
    "    # back prop equations for last element , last layer uses sigmoid\n",
    "    globals()['dZ' + str(L)] = (1/m)*((globals()['A' + str(L)]) - Y) # I have proved that, for softmax and sigmoid, this equation remains same\n",
    "    globals()['dW' + str(L)] = np.dot((globals()['dZ' + str(L)]), (globals()['A' + str(L -1)]).T)\n",
    "    globals()['db' + str(L)] = np.sum((globals()['dZ' + str(L)]), axis = 1, keepdims = True)\n",
    "    globals()['dA' + str(L-1)]  = np.dot((globals()['W' +str(i)]).T , (globals()['dZ' + str(L)]))\n",
    "    \n",
    "    # back prop equations for layers L-1, L-2, ....1\n",
    "    for l in range(L-1, 0, -1):\n",
    "        globals()['dZ' + str(l)] = np.multiply((globals()['dA' + str(l)]), np.int64((globals()['A' + str(l)])>0))\n",
    "        globals()['dW' + str(l)] = np.dot((globals()['dZ' + str(l)]), (globals()['A' + str(l -1)]).T)\n",
    "        globals()['db' + str(l)] = np.sum((globals()['dZ' + str(l)]), axis = 1, keepdims = True)\n",
    "        globals()['dA' + str(l-1)] = np.dot((globals()['W' +str(l)]).T , (globals()['dZ' + str(l)]))\n",
    "\n",
    "    # updating gradients in dictionary\n",
    "    gradients = {}\n",
    "    for i in range(1, L+1):\n",
    "        gradients[('dW' +str(i))] = globals()['dW' +str(i)] \n",
    "        gradients[('db' +str(i))] = globals()['db' +str(i)]\n",
    "    \n",
    "    return gradients    \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backward Propagation (linear -> relu ->linear ->relu.....->linear ->sigmoid) with Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# relu ->relu ->......->relu ->sigmoid     \n",
    "# cache has order ( Z1, A1, W1, b1, Z2,, A2, W2, b2...)  Please note that dZapp( apparant dZ is different from real dZ)\n",
    "def back_prop_with_regularization(X, Y, cache, lambd):   # dZapp means partial derivative of cross-entropy part\n",
    "    L = int((len(cache))/4)                                 # of cost with respect to Z for any layer l. This is a tricky approach. Trick works to avoid mathematical complexity, think why?\n",
    "    m = X.shape[1]\n",
    "    globals()['A'+ str(0)] = X\n",
    "    for i in range(1,L+1):   # retrieving the cache elements\n",
    "        globals()['Z'+ str(i)]  =  cache[4*i - 4]\n",
    "        globals()['A' + str(i)] =  cache[4*i - 3]\n",
    "        globals()['W' +str(i)]  =   cache[4*i - 2]\n",
    "        globals()['b' +str(i)]  =   cache[4*i - 1]\n",
    "    # back prop equations for last element , last layer uses sigmoid\n",
    "    globals()['dZapp' + str(L)] = (1/m)*((globals()['A' + str(L)]) - Y) # I have proved that, for softmax and sigmoid, this equation remains same\n",
    "    globals()['dW' + str(L)] = np.dot((globals()['dZapp' + str(L)]), (globals()['A' + str(L -1)]).T) + (lambd/m)*(globals()['W' +str(L)])\n",
    "    globals()['db' + str(L)] = np.sum((globals()['dZapp' + str(L)]), axis = 1, keepdims = True)\n",
    "    globals()['dA' + str(L-1)]  = np.dot((globals()['W' +str(i)]).T , (globals()['dZapp' + str(L)]))\n",
    "    \n",
    "    # back prop equations for layers L-1, L-2, ....1\n",
    "    for l in range(L-1, 0, -1):\n",
    "        globals()['dZapp' + str(l)] = np.multiply((globals()['dA' + str(l)]), np.int64((globals()['A' + str(l)])>0))\n",
    "        globals()['dW' + str(l)] = np.dot((globals()['dZapp' + str(l)]), (globals()['A' + str(l -1)]).T) + (lambd/m)*(globals()['W' +str(l)])\n",
    "        globals()['db' + str(l)] = np.sum((globals()['dZapp' + str(l)]), axis = 1, keepdims = True)\n",
    "        globals()['dA' + str(l-1)] = np.dot((globals()['W' +str(l)]).T , (globals()['dZapp' + str(l)]))\n",
    "\n",
    "    # updating gradients in dictionary\n",
    "    gradients = {}\n",
    "    for i in range(1, L+1):\n",
    "        gradients[('dW' +str(i))] = globals()['dW' +str(i)] \n",
    "        gradients[('db' +str(i))] = globals()['db' +str(i)]\n",
    "    \n",
    "    return gradients    \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backward Propagation (linear -> relu ->linear ->relu.....->linear ->sigmoid) with Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# relu ->relu ->......->relu ->sigmoid\n",
    "# cache has order ( Z1, DA1, A1, W1, b1, Z2, DA2, A2, W2, b2...., ZL, AL, WL, bL) ...last layer does not have mask DAL\n",
    "def back_prop_with_dropout(X, Y, cache, keep_prob):  \n",
    "    \n",
    "    L = int((len(cache))/5) + 1\n",
    "    m = X.shape[1]\n",
    "    globals()['A'+str(0)] = X\n",
    "    \n",
    "    globals()['DA' +str(0)] = np.zeros((X.shape[0], X.shape[1]))\n",
    "    \n",
    "    #Accessing cache elements for hidden layers\n",
    "    for i in range(1,L):   # retrieving the cache elements\n",
    "        globals()['Z'+ str(i)]  =  cache[5*i - 5]\n",
    "        globals()['DA' + str(i)] =  cache[5*i - 4]\n",
    "        globals()['A' + str(i)] =  cache[5*i - 3]\n",
    "        globals()['W' +str(i)]  =   cache[5*i - 2]\n",
    "        globals()['b' +str(i)]  =   cache[5*i - 1]\n",
    "    # Accessing cache elements for output layer \"L\"  \n",
    "    globals()['Z'+ str(L)]  =  cache[5*L - 5]\n",
    "    globals()['A' + str(L)] =  cache[5*L - 4]\n",
    "    globals()['W' +str(L)]  =   cache[5*L - 3]\n",
    "    globals()['b' +str(L)]  =   cache[5*L - 2]\n",
    "    \n",
    "    # back prop equations for last element , last layer uses sigmoid\n",
    "    globals()['dZ' + str(L)] = (1/m)*((globals()['A' + str(L)]) - Y) # I have proved that, for softmax and sigmoid, this equation remains same\n",
    "    \n",
    "    # masking of last hidden layer with keep_prob\n",
    "    #globals()['A'+ str(L-1)] = np.multiply(globals()['A' +str(L-1)], globals()['DA'+str(L-1)])\n",
    "    #globals()['A'+ str(L-1)] = (globals()['A'+ str(L-1)])/keep_prob\n",
    "    \n",
    "    globals()['dW' + str(L)] = np.dot((globals()['dZ' + str(L)]), (globals()['A' + str(L -1)]).T)\n",
    "    globals()['db' + str(L)] = np.sum((globals()['dZ' + str(L)]), axis = 1, keepdims = True)\n",
    "    \n",
    "    # masking of dA of last hidden layer\n",
    "    globals()['dA' + str(L-1)]  = np.dot((globals()['W' +str(L)]).T , (globals()['dZ' + str(L)]))\n",
    "    globals()['dA'+ str(L-1)] = np.multiply(globals()['dA' +str(L-1)], globals()['DA'+str(L-1)])\n",
    "    globals()['dA'+ str(L-1)] = (globals()['dA'+ str(L-1)])/keep_prob\n",
    "    \n",
    "    # back prop equations for layers L-1, L-2, ....1\n",
    "    for l in range(L-1, 0, -1):\n",
    "        # transform A\n",
    "        \n",
    "        globals()['dZ' + str(l)] = np.multiply((globals()['dA' + str(l)]), np.int64((globals()['A' + str(l)])>0))\n",
    "        \n",
    "        #globals()['A'+ str(l-1)] = np.multiply(globals()['A' +str(l-1)], globals()['DA'+str(l-1)])\n",
    "        #globals()['A'+ str(l-1)] = (globals()['A'+ str(l-1)])/keep_prob\n",
    "        \n",
    "        globals()['dW' + str(l)] = np.dot((globals()['dZ' + str(l)]), (globals()['A' + str(l -1)]).T)\n",
    "        globals()['db' + str(l)] = np.sum((globals()['dZ' + str(l)]), axis = 1, keepdims = True)\n",
    "        \n",
    "        globals()['dA' + str(l-1)] = np.dot((globals()['W' +str(l)]).T , (globals()['dZ' + str(l)]))\n",
    "        \n",
    "        globals()['dA'+ str(l-1)] = np.multiply(globals()['dA' +str(l-1)], globals()['DA'+str(l-1)])\n",
    "        globals()['dA'+ str(l-1)] = (globals()['dA'+ str(l-1)])/keep_prob\n",
    "\n",
    "    # updating gradients in dictionary\n",
    "    gradients = {}\n",
    "    for i in range(1, L+1):\n",
    "        gradients[('dW' +str(i))] = globals()['dW' +str(i)] \n",
    "        gradients[('db' +str(i))] = globals()['db' +str(i)]\n",
    "    \n",
    "    return gradients    \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict Accuracy with Sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_accuracy(X, Y, parameters):\n",
    "    m = Y.shape[1]\n",
    "    AL,cache = forward_prop(X,parameters)\n",
    "    AL_predicted = (AL > 0.5)\n",
    "    accuracy = np.sum(np.float32(AL_predicted == Y))/m\n",
    "    \n",
    "    return accuracy\n",
    "    \n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict Accuracy with dropout Sigmoid ( For Training set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_accuracy_with_dropout(X, Y, parameters, keep_prob):\n",
    "    m = Y.shape[1]\n",
    "    AL,cache = forward_prop_with_dropout(X,parameters, keep_prob)\n",
    "    AL_predicted = (AL > 0.5)\n",
    "    accuracy = np.sum(np.float32(AL_predicted == Y))/m\n",
    "    \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OPTIMIZERS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent( Batch Gradient Descent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent_update(parameters, gradients, learning_rate):\n",
    "    L = int(len(parameters)/2)\n",
    "    for l in range (1,L+1):\n",
    "        parameters['W' +str(l)] = parameters['W' + str(l)] - learning_rate*gradients['dW' + str(l)]\n",
    "        parameters['b' +str(l)] = parameters['b' + str(l)] - learning_rate*gradients['db' +str(l)]\n",
    "    return parameters   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Random Mini Batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_minibatches(X_train, Y_train, minibatch_size):\n",
    "    m = X_train.shape[1]\n",
    "    # To shuffle X and Y train.\n",
    "    #np.random.seed(0)\n",
    "    K = list(np.random.permutation(m))  # k is an array, list() changes an array into a list. https://docs.scipy.org/doc/numpy/reference/generated/numpy.random.permutation.html\n",
    "    shuffled_X = X_train[:,K]\n",
    "    shuffled_Y = Y_train[:,K].reshape((1,m))\n",
    "    \n",
    "    \n",
    "    \n",
    "    minibatches = []\n",
    "    num_complete_minibatches = int(np.floor(m/minibatch_size))\n",
    "    for k in range(0, num_complete_minibatches):\n",
    "        minibatch_X = shuffled_X[:, k*minibatch_size:(k+1)*minibatch_size]\n",
    "        minibatch_Y = shuffled_Y[:,k*minibatch_size:(k+1)*minibatch_size]\n",
    "        minibatch = (minibatch_X, minibatch_Y)\n",
    "        minibatches.append(minibatch)\n",
    "        \n",
    "    # end case of mini batch\n",
    "    if m % minibatch_size != 0:\n",
    "        minibatch_X = shuffled_X[:,num_complete_minibatches*minibatch_size:m]\n",
    "        minibatch_Y = shuffled_Y[:,num_complete_minibatches*minibatch_size:m]\n",
    "        minibatch = (minibatch_X, minibatch_Y)\n",
    "        minibatches.append(minibatch)\n",
    "    \n",
    "    return minibatches    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Momentum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Velocities "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_velocity(parameters):\n",
    "    \n",
    "    L = int((1/2)*len(parameters))\n",
    "    V = {}\n",
    "    for l in range(1, L+1):\n",
    "        V['dW'+ str(l)] = np.zeros(((parameters['W'+str(l)]).shape))\n",
    "        V['db'+ str(l)] = np.zeros(((parameters['b'+str(l)]).shape))\n",
    "    \n",
    "    return V     \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update Parameters with Momentum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def momentum_update(parameters, gradients, learning_rate, V, beta1):\n",
    "    \n",
    "    L = int((1/2)*len(parameters))\n",
    "    for l in range(1,L+1):\n",
    "        V['dW'+str(l)] = beta1*V['dW'+str(l)] + (1-beta1)*gradients['dW'+str(l)]\n",
    "        V['db'+str(l)] = beta1*V['db'+str(l)] + (1-beta1)*gradients['db'+str(l)]\n",
    "        \n",
    "        # parameters update\n",
    "        parameters['W'+str(l)] = parameters['W'+str(l)] - learning_rate*V['dW'+str(l)]\n",
    "        parameters['b'+str(l)] = parameters['b'+str(l)] - learning_rate*V['db'+str(l)]\n",
    "        \n",
    "    return parameters, V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RMS Prop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize RMS Prop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_rms_prop(parameters):\n",
    "    \n",
    "    L = int((1/2)*len(parameters))\n",
    "    S = {}\n",
    "    for l in range(1, L+1):\n",
    "        S['dW'+ str(l)] = np.zeros(((parameters['W'+str(l)]).shape))\n",
    "        S['db'+ str(l)] = np.zeros(((parameters['b'+str(l)]).shape))\n",
    "    \n",
    "    return S     \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update Parameters with RMS Prop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rms_prop_update(parameters, gradients, learning_rate, S, beta2, epsilon = 1e-8):\n",
    "    \n",
    "    L = int((1/2)*len(parameters))\n",
    "    \n",
    "    for l in range(1, L+1):\n",
    "        S['dW'+str(l)] = beta2*S['dW'+str(l)] + (1-beta2)*np.square(gradients['dW'+str(l)])\n",
    "        S['db' + str(l)] = beta2*S['db' +str(l)] + (1-beta2)*np.square(gradients['db' +str(l)])\n",
    "        \n",
    "        #parameters update\n",
    "        parameters['W'+str(l)] = parameters['W' +str(l)] - (learning_rate/(np.sqrt(S['dW'+str(l)])+ epsilon))*gradients['dW'+str(l)]\n",
    "        parameters['b'+str(l)] = parameters['b' +str(l)] - (learning_rate/(np.sqrt(S['db'+str(l)])+ epsilon))*gradients['db'+str(l)]\n",
    "        \n",
    "    return parameters, S    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ADAM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adam Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_adam(parameters):\n",
    "    \n",
    "    L = int((1/2)*len(parameters))\n",
    "    V= {}\n",
    "    S = {}\n",
    "    for l in range(1, L+1):\n",
    "        V['dW'+ str(l)] = np.zeros(((parameters['W'+str(l)]).shape))\n",
    "        V['db'+ str(l)] = np.zeros(((parameters['b'+str(l)]).shape))\n",
    "        S['dW'+ str(l)] = np.zeros(((parameters['W'+str(l)]).shape))\n",
    "        S['db'+ str(l)] = np.zeros(((parameters['b'+str(l)]).shape))\n",
    "        \n",
    "    return V, S    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update Parameters with Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adam_update(parameters, gradients, learning_rate, V, S, t,  beta1 = 0.9, beta2 = 0.999, epsilon = 1e-8):\n",
    "    \n",
    "    L = int((1/2)*len(parameters))\n",
    "    V_corrected = {}\n",
    "    S_corrected = {}\n",
    "    \n",
    "    for l in range(1, L+1):\n",
    "        \n",
    "        V['dW'+str(l)] = beta1*V['dW'+str(l)] + (1-beta1)*gradients['dW'+str(l)]\n",
    "        V_corrected['dW'+str(l)] = V['dW'+str(l)]/(1- (beta1)**t)\n",
    "        \n",
    "        V['db'+str(l)] = beta1*V['db'+str(l)] + (1-beta1)*gradients['db'+str(l)]\n",
    "        V_corrected['db'+str(l)] = V['db'+str(l)]/(1- (beta1)**t)\n",
    "        \n",
    "        S['dW'+str(l)] = beta2*S['dW'+str(l)] + (1-beta2)*np.square(gradients['dW'+str(l)])\n",
    "        S_corrected['dW'+str(l)] = S['dW'+str(l)]/(1- (beta2)**t)\n",
    "        \n",
    "        S['db'+str(l)] = beta2*S['db'+str(l)] + (1-beta2)*np.square(gradients['db'+str(l)])\n",
    "        S_corrected['db'+str(l)] = S['db'+str(l)]/(1- (beta2)**t)\n",
    "        \n",
    "        #parameters update\n",
    "        parameters['W'+str(l)] = parameters['W'+str(l)] - (learning_rate/(np.sqrt(S_corrected['dW'+str(l)]) + epsilon))*V_corrected['dW'+str(l)]\n",
    "        parameters['b'+str(l)] = parameters['b'+str(l)] - (learning_rate/(np.sqrt(S_corrected['db'+str(l)]) + epsilon))*V_corrected['db'+str(l)]\n",
    "    \n",
    "    return parameters, V, S"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sigmoid  Model with Regularization_OR_Dropout (with different Optimizers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this definition allows only to use regularized or dropout at a time although it is possible to use both at the same time.\n",
    "def regularized_or_dropout_optimizers_model_sigmoid(X, Y, layers_dims, minibatch_size, lambd, keep_prob, optimizer, num_epochs, learning_rate, beta1, beta2, epsilon):\n",
    "    \n",
    "    #initialize parameters\n",
    "    parameters = He_initialize_parameters(layers_dims)\n",
    "    costs = []\n",
    "    \n",
    "    # initialize adam counter\n",
    "    t = 0\n",
    "    # initialize optimizers\n",
    "    if optimizer == 'gd':\n",
    "        pass\n",
    "    elif optimizer == 'momentum':\n",
    "        V = initialize_velocity(parameters)\n",
    "    elif optimizer == 'rms_prop':\n",
    "        S = initialize_rms_prop(parameters) \n",
    "    elif optimizer == 'adam':\n",
    "        V, S = initialize_adam(parameters)\n",
    "    \n",
    "    \n",
    "    if lambd != 0:\n",
    "        for i in range(0, num_epochs):\n",
    "            minibatches = random_minibatches(X, Y, minibatch_size)\n",
    "            for minibatch in minibatches:\n",
    "                (minibatch_X, minibatch_Y) = minibatch\n",
    "                AL, cache  = forward_prop(minibatch_X, parameters)\n",
    "                grads  = back_prop_with_regularization(minibatch_X, minibatch_Y, cache, lambd)\n",
    "                \n",
    "                if optimizer  == 'gd':\n",
    "                    parameters = gradient_descent_update(parameters, grads, learning_rate)\n",
    "                elif optimizer == 'momentum':\n",
    "                    parameters, V = momentum_update(parameters, grads, learning_rate,V, beta1)\n",
    "                elif optimizer == 'rms_prop':\n",
    "                    parameters, S = rms_prop_update(parameters, grads, learning_rate, S, beta2, epsilon)\n",
    "                elif optimizer == 'adam':\n",
    "                    t = t+1  # increment adam counter... I have to know why to do this\n",
    "                    parameters, V, S == adam_update(parameters, grads, learning_rate, V, S, t, beta1, beta2, epsilon)\n",
    "                \n",
    "                cost = regularized_compute_cost_sigmoid(AL, minibatch_Y, parameters, lambd)\n",
    "        \n",
    "            if i%100==0:\n",
    "                print('cost after ' +str(i) +' epochs is ' + str(cost))\n",
    "            if i%10 ==0:\n",
    "                costs.append(cost)\n",
    "        \n",
    "        plt.plot(np.squeeze(costs))\n",
    "        plt.ylabel('cost')\n",
    "        plt.xlabel('iterations (per tens)')\n",
    "        plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "        plt.show()\n",
    "        \n",
    "    elif keep_prob !=0:\n",
    "        for i in range(0, num_epochs):\n",
    "            minibatches = random_minibatches(X, Y, minibatch_size)\n",
    "            for minibatch in minibatches:\n",
    "                (minibatch_X, minibatch_Y) = minibatch\n",
    "                AL, cache  = forward_prop_with_dropout(minibatch_X, parameters, keep_prob)\n",
    "                grads  = back_prop_with_dropout(minibatch_X, minibatch_Y, cache, keep_prob)\n",
    "                \n",
    "                if optimizer  == 'gd':\n",
    "                    parameters = gradient_descent_update(parameters, grads, learning_rate)\n",
    "                elif optimizer == 'momentum':\n",
    "                    parameters, V = momentum_update(parameters, grads, learning_rate,V, beta1)\n",
    "                elif optimizer == 'rms_prop':\n",
    "                    parameters, S = rms_prop_update(parameters, grads, learning_rate, S, beta2, epsilon)\n",
    "                elif optimizer == 'adam':\n",
    "                    t = t+1  # increment adam counter... I have to know why to do this\n",
    "                    parameters, V, S == adam_update(parameters, grads, learning_rate, V, S, t, beta1, beta2, epsilon)\n",
    "                \n",
    "                cost = compute_cost_sigmoid(AL, minibatch_Y)\n",
    "        \n",
    "            if i%100==0:\n",
    "                print('cost after ' +str(i) +' epochs is ' + str(cost))\n",
    "            if i%10 ==0:\n",
    "                costs.append(cost)\n",
    "        \n",
    "        plt.plot(np.squeeze(costs))\n",
    "        plt.ylabel('cost')\n",
    "        plt.xlabel('iterations (per tens)')\n",
    "        plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "        plt.show()\n",
    "    \n",
    "    elif (lambd== 0) and (keep_prob ==0):\n",
    "        for i in range(0, num_epochs):\n",
    "            minibatches = random_minibatches(X, Y, minibatch_size)\n",
    "            for minibatch in minibatches:\n",
    "                (minibatch_X, minibatch_Y) = minibatch\n",
    "                AL, cache  = forward_prop(minibatch_X, parameters)\n",
    "                grads  = back_prop_without_regularization(minibatch_X, minibatch_Y, cache)\n",
    "                \n",
    "                if optimizer  == 'gd':\n",
    "                    parameters = gradient_descent_update(parameters, grads, learning_rate)\n",
    "                elif optimizer == 'momentum':\n",
    "                    parameters, V = momentum_update(parameters, grads, learning_rate,V, beta1)\n",
    "                elif optimizer == 'rms_prop':\n",
    "                    parameters, S = rms_prop_update(parameters, grads, learning_rate, S, beta2, epsilon)\n",
    "                elif optimizer == 'adam':\n",
    "                    t = t+1  # increment adam counter... I have to know why to do this\n",
    "                    parameters, V, S == adam_update(parameters, grads, learning_rate, V, S, t, beta1, beta2, epsilon)\n",
    "                \n",
    "                cost = compute_cost_sigmoid(AL, minibatch_Y)\n",
    "            \n",
    "            if i%100==0:\n",
    "                print('cost after ' +str(i) +' epochs is ' + str(cost))\n",
    "            if i%10 ==0:\n",
    "                costs.append(cost) \n",
    "       \n",
    "        plt.plot(np.squeeze(costs))\n",
    "        plt.ylabel('cost')\n",
    "        plt.xlabel('iterations (per tens)')\n",
    "        plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "        plt.show()\n",
    "        \n",
    "        \n",
    "    return parameters  \n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
