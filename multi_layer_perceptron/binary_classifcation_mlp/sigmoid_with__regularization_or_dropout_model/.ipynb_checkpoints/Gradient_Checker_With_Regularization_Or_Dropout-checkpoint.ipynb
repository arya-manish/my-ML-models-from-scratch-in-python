{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Checker (for Basic Sigmoid no regularization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ReLu ( needed for forward prop in grad checker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ReLu\n",
    "def relu(x):\n",
    "    \n",
    "   #  Arguments:\n",
    "   #x -- A scalar or numpy array of any size.\n",
    "    \n",
    "    r = np.maximum(0,x)\n",
    "    \n",
    "    return r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sigmoid( needed for forward prop in grad checker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sigmoid\n",
    "def sigmoid(x):\n",
    "    \n",
    "    #  Arguments:\n",
    "    #x -- A scalar or numpy array of any size.\n",
    "    \n",
    "    s = 1/(1+np.exp(-x))\n",
    "    return s\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute Cost for sigmoid (needed for Grad Checker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost_sigmoid(AL, Y):\n",
    "    m = Y.shape[1]\n",
    "    J = -(1/m)*np.nansum((np.multiply(Y, np.log(AL)) + np.multiply((1 - Y), np.log(1 - AL))))                                   \n",
    "    return J"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regularized Compute Cost   (needed for Grad Checker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regularized_compute_cost_sigmoid(AL, Y, parameters, lambd):\n",
    "    m = Y.shape[1]\n",
    "    L = int((1/2)*len(parameters))\n",
    "    sumWsquared = 0.\n",
    "    for l in range(1,L+1):\n",
    "        sumWsquared = sumWsquared + np.sum(np.square(parameters['W'+str(l)]))\n",
    "    J_Cross_Entropy = -(1/m)*np.sum((np.multiply(Y, np.log(AL)) + np.multiply((1 - Y), np.log(1 - AL))))\n",
    "    J_Regularized_term = (lambd/(2*m))*sumWsquared\n",
    "    J = J_Cross_Entropy + J_Regularized_term                                    \n",
    "    return J"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Basic Forward prop for both non-regularization and regularization (needed for Grad Checker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_prop(X, parameters):   # linear -> relu ->linear ->relu.....->linear ->sigmoid\n",
    "    \n",
    "    L = int((1/2)*len(parameters))\n",
    "    globals()['A'+ str(0)] = X\n",
    "    for l in range(1, L):  # linear->relu upto L-1 layer\n",
    "        globals()['Z'+ str(l)] = np.dot(parameters['W'+str(l)], (globals()['A'+ str(l-1)])) + parameters['b' +str(l)]\n",
    "        globals()['A'+ str(l)] = relu((globals()['Z'+ str(l)]))\n",
    "    \n",
    "    globals()['Z'+ str(L)] = np.dot(parameters['W'+str(L)], (globals()['A'+ str(L-1)])) + parameters['b' +str(L)] \n",
    "    globals()['A'+ str(L)] = sigmoid((globals()['Z'+ str(L)]))                                \n",
    "    # storing values in cache\n",
    "    lst = []   # creating list and adding Z1, A1, W1, b1... then converting it into a tuple\n",
    "    \n",
    "    for l in range(1,L+1):                                  \n",
    "        lst.append(globals()['Z'+ str(l)])\n",
    "        lst.append(globals()['A'+ str(l)])                              \n",
    "        lst.append(parameters['W' +str(l)])\n",
    "        lst.append(parameters['b' +str(l)]) \n",
    "    cache = tuple(lst)\n",
    "    \n",
    "    return (globals()['A'+ str(L)]), cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### Forward prop with dropout (needed for Grad Checker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_prop_with_dropout(X, parameters, keep_prob):   # linear -> relu ->linear ->relu.....->linear ->sigmoid\n",
    "    \n",
    "    np.random.seed(1)\n",
    "    \n",
    "    L = int((1/2)*len(parameters))\n",
    "    globals()['A'+ str(0)] = X\n",
    "    for l in range(1, L):  # linear->relu upto L-1 layer\n",
    "        globals()['Z'+ str(l)] = np.dot(parameters['W'+str(l)], (globals()['A'+ str(l-1)])) + parameters['b' +str(l)]\n",
    "        globals()['A'+ str(l)] = relu((globals()['Z'+ str(l)]))\n",
    "        # dropout equations on the hidden layers and not on the input and output layers\n",
    "        globals()['DA' + str(l)] = np.random.rand((globals()['A'+str(l)]).shape[0],(globals()['A'+str(l)]).shape[1]) # DA1, DA2 etc. stands for dropout A1, A2 etc   \n",
    "        globals()['DA' + str(l)] = globals()['DA' + str(l)] < keep_prob\n",
    "        globals()['A'+ str(l)] = np.multiply(globals()['A'+ str(l)], globals()['DA' + str(l)])\n",
    "        globals()['A'+ str(l)] =  (globals()['A'+ str(l)]) /keep_prob\n",
    "        \n",
    "    globals()['Z'+ str(L)] = np.dot(parameters['W'+str(L)], (globals()['A'+ str(L-1)])) + parameters['b' +str(L)] \n",
    "    globals()['A'+ str(L)] = sigmoid((globals()['Z'+ str(L)]))                                \n",
    "    # storing values in cache\n",
    "    lst = []   # creating list and adding Z1, A1, W1, b1... then converting it into a tuple\n",
    "    \n",
    "    for l in range(1,L):                                  \n",
    "        lst.append(globals()['Z'+ str(l)])\n",
    "        lst.append(globals()['DA'+ str(l)])\n",
    "        lst.append(globals()['A'+ str(l)])\n",
    "        lst.append(parameters['W' +str(l)])\n",
    "        lst.append(parameters['b' +str(l)])\n",
    "    lst.append(globals()['Z'+ str(L)])\n",
    "    lst.append(globals()['A'+ str(L)])\n",
    "    lst.append(parameters['W' +str(L)])\n",
    "    lst.append(parameters['b' +str(L)])     \n",
    "    cache = tuple(lst)\n",
    "    \n",
    "    return (globals()['A'+ str(L)]), cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dictionary(Parameters) To Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dictionary_to_vector(parameters):\n",
    "    L = len(parameters)//2\n",
    "    \n",
    "    for l in range(1,L+1):\n",
    "        globals()['W_vec' +str(l)] = np.reshape(parameters['W' + str(l)], (-1,1))  \n",
    "        globals()['b_vec' +str(l)] = np.reshape(parameters['b' + str(l)], (-1,1)) \n",
    "        new_vec = np.concatenate((globals()['W_vec'+ str(l)], globals()['b_vec'+ str(l)]), axis = 0 )\n",
    "        if l==1 :\n",
    "            theta = new_vec\n",
    "        else:    \n",
    "            theta = np.concatenate((theta, new_vec), axis = 0)\n",
    "    \n",
    "    # returning parameter dimensions as list to get back from vector to parameters\n",
    "    param_dims = []\n",
    "    for l in range (1,L+1):\n",
    "        param_dims.append(parameters['W' + str(l)].shape)\n",
    "        param_dims.append((parameters['W' + str(l)].shape[0])*(parameters['W' + str(l)].shape[1]))\n",
    "        param_dims.append(parameters['b' + str(l)].shape)\n",
    "        param_dims.append((parameters['b' + str(l)].shape[0])*(parameters['b' + str(l)].shape[1]))\n",
    "    \n",
    "    return theta, param_dims"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vector to Dictionary (Parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Vector_to_dictionary\n",
    "\n",
    "def vector_to_dictionary(theta, param_dims):\n",
    "    parameters = {}\n",
    "    L = len(param_dims)//4\n",
    "    \n",
    "     \n",
    "    for l in range(1, L+1):\n",
    "        \n",
    "         \n",
    "        # to get parameter elements, indexW refers to indices of W's of param_dims\n",
    "        indexW = 1 + (l-1)*4   # n th element of an AP, to get W1, W2 etc\n",
    "        if indexW == 1:\n",
    "            parameters['W'+str(l)] = theta[0:param_dims[indexW]].reshape((param_dims[4*(l-1)]))\n",
    "        \n",
    "        else:\n",
    "            vec_index = 0\n",
    "            for x in range(1,indexW , 2):   # adding before the current index of param_dims\n",
    "                vec_index = vec_index + param_dims[x]\n",
    "            parameters['W'+str(l)] = theta[vec_index: vec_index + param_dims[indexW]].reshape((param_dims[4*(l-1)]))\n",
    "        # to get parameter elements, indexb refers to indices of b's of param_dims\n",
    "        \n",
    "        indexb = 3 + (l-1)*4\n",
    "        if indexb == 3:\n",
    "            parameters['b'+str(l)] = theta[param_dims[indexW]: param_dims[indexW] + param_dims[indexb]].reshape((param_dims[2+ 4*(l-1)])) \n",
    "        else:\n",
    "            vec_index = 0  \n",
    "            for x in range(1, indexb, 2):\n",
    "                vec_index = vec_index + param_dims[x]\n",
    "            parameters['b'+str(l)] = theta[vec_index: vec_index + param_dims[indexb]].reshape((param_dims[2+ 4*(l-1)])) \n",
    "    \n",
    "    return  parameters                                   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradients (Dictionary) to Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Gradients_to_vectors\n",
    "def gradients_to_vector(gradients):\n",
    "    L = np.int(len(gradients)/2)\n",
    "\n",
    "    \n",
    "    for l in range(1,L+1):\n",
    "        globals()['dW_vec' +str(l)] = np.reshape(gradients['dW' + str(l)], (-1,1))  \n",
    "        globals()['db_vec' +str(l)] = np.reshape(gradients['db' + str(l)], (-1,1)) \n",
    "        new_vec = np.concatenate((globals()['dW_vec'+ str(l)], globals()['db_vec'+ str(l)]), axis = 0 )\n",
    "        if l==1 :\n",
    "            theta_grad = new_vec\n",
    "        else:    \n",
    "            theta_grad = np.concatenate((theta_grad, new_vec), axis = 0)\n",
    "    \n",
    "    return theta_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient Checker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_checker_with_regularization_or_dropout(X, Y, parameters, gradients, lambd, keep_prob, epsilon = 1e-7):\n",
    "    theta, param_dims = dictionary_to_vector(parameters)\n",
    "    num_parameters = theta.shape[0]\n",
    "    Jplus = np.zeros((num_parameters,1))\n",
    "    Jminus = np.zeros((num_parameters,1))\n",
    "    grad_approx = np.zeros((num_parameters,1))\n",
    "    grad = gradients_to_vector(gradients)\n",
    "    \n",
    "    for i in range (num_parameters):\n",
    "        \n",
    "        thetaplus = np.copy(theta)\n",
    "        thetaplus[i][0] = thetaplus[i][0] + epsilon\n",
    "        parameters_new = vector_to_dictionary(thetaplus, param_dims)\n",
    "        \n",
    "        if lambd != 0:\n",
    "            ALplus, _ = forward_prop(X, parameters_new)\n",
    "            Jplus[i] = regularized_compute_cost_sigmoid(ALplus, Y, parameters_new, lambd)\n",
    "        elif keep_prob != 0:\n",
    "            ALplus, _ = forward_prop_with_dropout(X, parameters_new, keep_prob)\n",
    "            Jplus[i] =  compute_cost_sigmoid(ALplus, Y) \n",
    "        elif lambd==0 and keep_prob ==0:\n",
    "            ALplus, _ = forward_prop(X, parameters_new)\n",
    "            Jplus[i] =  compute_cost_sigmoid(ALplus, Y)\n",
    "        \n",
    "        thetaminus = np.copy(theta)\n",
    "        thetaminus[i][0] = thetaminus[i][0] - epsilon\n",
    "        parameters_new = vector_to_dictionary(thetaminus, param_dims)\n",
    "        \n",
    "        if lambd != 0:\n",
    "            ALminus, _ = forward_prop(X, parameters_new)\n",
    "            Jminus[i] = regularized_compute_cost_sigmoid(ALminus, Y, parameters_new, lambd)\n",
    "        elif keep_prob != 0:\n",
    "            ALminus, _ = forward_prop_with_dropout(X, parameters_new, keep_prob)\n",
    "            Jminus[i] =  compute_cost_sigmoid(ALminus, Y) \n",
    "        elif lambd==0 and keep_prob ==0:\n",
    "            ALminus, _ = forward_prop(X, parameters_new)\n",
    "            Jminus[i] =  compute_cost_sigmoid(ALminus, Y)\n",
    "        \n",
    "        grad_approx[i] = (Jplus[i] - Jminus[i])/(2*epsilon)\n",
    "        \n",
    "        print('gradient using grad checker is' +str(grad_approx[i]))\n",
    "        print('gradient using back prop is' +str(grad[i]))\n",
    "    \n",
    "    numerator = np.linalg.norm(grad - grad_approx)\n",
    "   \n",
    "        \n",
    "    denominator = np.linalg.norm(grad) + np.linalg.norm(grad_approx)\n",
    "     \n",
    "        \n",
    "    difference = numerator/denominator\n",
    "        \n",
    "    if difference < 2e-7:\n",
    "        print('Your back prop is working absolutely fine. Difference is ' + str(difference))\n",
    "    if difference > 2e-7:\n",
    "        print('check your back prop..Error in back prop. Differnece is ' + str(difference))\n",
    "\n",
    "    return difference    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
